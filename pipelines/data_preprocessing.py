import os
import pickle
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import argparse

def preprocess_data(output_path="data"):
    print("ğŸ”„ Chargement des donnÃ©es Iris...")
    
    # Charger les donnÃ©es Iris
    iris = load_iris()
    X, y = iris.data, iris.target
    
    print(f"ğŸ“Š Dataset: {X.shape[0]} Ã©chantillons, {X.shape[1]} features")
    print(f"ğŸ“Š Classes: {iris.target_names}")
    
    # S'assurer que les noms sont des listes Python (pas des numpy arrays)
    feature_names = list(iris.feature_names) if hasattr(iris.feature_names, '__iter__') else iris.feature_names
    target_names = list(iris.target_names) if hasattr(iris.target_names, '__iter__') else iris.target_names
    
    print("ğŸ”„ Division train/test (80/20)...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42,
        stratify=y
    )
    
    print("ğŸ”„ Normalisation des features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # CrÃ©er le dossier de sortie
    os.makedirs(output_path, exist_ok=True)
    
    print(f"ğŸ’¾ Sauvegarde des donnÃ©es dans {output_path}...")
    
    # Sauvegarder les donnÃ©es d'entraÃ®nement
    with open(os.path.join(output_path, 'X_train.pkl'), 'wb') as f:
        pickle.dump(X_train_scaled, f)
    
    with open(os.path.join(output_path, 'X_test.pkl'), 'wb') as f:
        pickle.dump(X_test_scaled, f)
    
    with open(os.path.join(output_path, 'y_train.pkl'), 'wb') as f:
        pickle.dump(y_train, f)
    
    with open(os.path.join(output_path, 'y_test.pkl'), 'wb') as f:
        pickle.dump(y_test, f)
    
    # Sauvegarder le scaler
    with open(os.path.join(output_path, 'scaler.pkl'), 'wb') as f:
        pickle.dump(scaler, f)
    
    # PrÃ©parer les mÃ©tadonnÃ©es - CORRECTION ICI
    metadata = {
        "feature_names": feature_names,  # Pas de .tolist() car c'est dÃ©jÃ  une liste
        "target_names": target_names,    # Pas de .tolist() car c'est dÃ©jÃ  une liste
        "n_features": X.shape[1],
        "n_classes": len(np.unique(y)),
        "train_size": len(X_train),
        "test_size": len(X_test),
        "feature_stats": {
            "mean": X_train_scaled.mean(axis=0).tolist(),
            "std": X_train_scaled.std(axis=0).tolist()
        }
    }
    
    # Sauvegarder les mÃ©tadonnÃ©es
    with open(os.path.join(output_path, 'metadata.pkl'), 'wb') as f:
        pickle.dump(metadata, f)
    
    print("âœ… Preprocessing terminÃ© avec succÃ¨s!")
    print(f"ğŸ“Š DonnÃ©es d'entraÃ®nement: {X_train_scaled.shape}")
    print(f"ğŸ“Š DonnÃ©es de test: {X_test_scaled.shape}")
    print(f"ğŸ¯ Classes: {len(np.unique(y))}")
    print(f"ğŸ“‹ Features: {feature_names}")
    print(f"ğŸ“‹ Classes: {target_names}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PrÃ©processing des donnÃ©es Iris")
    parser.add_argument("--output_path", type=str, default="data",
                       help="Chemin de sortie pour les donnÃ©es preprocessÃ©es")
    
    args = parser.parse_args()
    preprocess_data(args.output_path)